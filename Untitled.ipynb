{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6c69e1-3ef0-4249-a59e-276278c9fa71",
   "metadata": {},
   "source": [
    "1. Importation des Bibliothèques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14df565-0a97-4f19-9e6f-0a88e363487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import sounddevice as sd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5f7e5-cd4f-4646-8f8a-7c64e0dcef16",
   "metadata": {},
   "source": [
    "1. Prétraitement des fichiers audio\n",
    "\n",
    "Il faut convertir les fichiers audio en caractéristiques exploitables par le réseau de neurones. Une approche courante est d'extraire des MFCC (Mel-Frequency Cepstral Coefficients), qui capturent les caractéristiques audio essentielles.\n",
    "\n",
    "Exemple de code pour extraire des MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24d91c7-a631-4de7-a8fd-48d9b57027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, max_pad_len=100):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        pad_width = max_pad_len - mfcc.shape[1]\n",
    "        if pad_width > 0:\n",
    "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_pad_len]\n",
    "        return mfcc.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec le fichier {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb6ece-57f0-4d69-ac0f-d25eaa13dcf2",
   "metadata": {},
   "source": [
    "2. Préparer le jeu de données\n",
    "\n",
    "Chargez les fichiers audio des commandes \"allumer\" et \"éteindre\", extrayez leurs caractéristiques, et préparez les labels correspondants.\n",
    "\n",
    "Exemple de préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ea4655-a5a1-41e3-a1db-c80eb5cbef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dossiers contenant les fichiers audio\n",
    "allumer_folder = \"C:/Users/flavi/projet/allumer_wav\"\n",
    "eteindre_folder = \"C:/Users/flavi/projet/eteindre_wav\"\n",
    "\n",
    "# Extraire les caractéristiques et créer les labels\n",
    "X, y = [], []\n",
    "\n",
    "# Pour \"allumer\"\n",
    "for file in os.listdir(allumer_folder):\n",
    "    if file.endswith(\".wav\"):\n",
    "        mfcc = extract_mfcc(os.path.join(allumer_folder, file))\n",
    "        if mfcc is not None:\n",
    "            X.append(mfcc)\n",
    "            y.append(\"allumer\")\n",
    "\n",
    "# Pour \"éteindre\"\n",
    "for file in os.listdir(eteindre_folder):\n",
    "    if file.endswith(\".wav\"):\n",
    "        mfcc = extract_mfcc(os.path.join(eteindre_folder, file))\n",
    "        if mfcc is not None:\n",
    "            X.append(mfcc)\n",
    "            y.append(\"eteindre\")\n",
    "\n",
    "# Convertir en tableaux numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encoder les labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # \"allumer\" -> 0, \"eteindre\" -> 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450105d6-2fd2-4a84-93a7-9d8d94bbccfe",
   "metadata": {},
   "source": [
    "3. Équilibrer les Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14f1e8f-cae7-4314-812d-2d6c36bb63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices des échantillons pour chaque classe\n",
    "allumer_indices = np.where(y == 0)[0]\n",
    "eteindre_indices = np.where(y == 1)[0]\n",
    "\n",
    "# Sous-échantillonner la classe majoritaire (\"éteindre\")\n",
    "eteindre_downsampled = resample(eteindre_indices, replace=False, n_samples=len(allumer_indices), random_state=42)\n",
    "\n",
    "# Combiner les indices équilibrés\n",
    "balanced_indices = np.concatenate([allumer_indices, eteindre_downsampled])\n",
    "\n",
    "# Créer les nouvelles données équilibrées\n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc562b-e8a2-4080-b6d6-0681cfa4857b",
   "metadata": {},
   "source": [
    "4. Diviser les données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2e6145-fbc1-490f-bf3c-bbf9c6c0ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167b3e3-6269-44f5-a972-7effbef04aee",
   "metadata": {},
   "source": [
    "6. Créer et entraîner le modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e63c20-6482-4bcd-804d-9991b3c481b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "941/941 [==============================] - 4s 3ms/step - loss: 0.2174 - accuracy: 0.9162 - val_loss: 0.1043 - val_accuracy: 0.9586\n",
      "Epoch 2/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.1023 - accuracy: 0.9601 - val_loss: 0.0617 - val_accuracy: 0.9689\n",
      "Epoch 3/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0677 - accuracy: 0.9735 - val_loss: 0.0644 - val_accuracy: 0.9713\n",
      "Epoch 4/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0568 - accuracy: 0.9782 - val_loss: 0.0461 - val_accuracy: 0.9793\n",
      "Epoch 5/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0445 - accuracy: 0.9843 - val_loss: 0.0427 - val_accuracy: 0.9827\n",
      "Epoch 6/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0465 - accuracy: 0.9835 - val_loss: 0.0388 - val_accuracy: 0.9806\n",
      "Epoch 7/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0372 - accuracy: 0.9869 - val_loss: 0.0312 - val_accuracy: 0.9862\n",
      "Epoch 8/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.0276 - val_accuracy: 0.9912\n",
      "Epoch 9/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 0.0437 - val_accuracy: 0.9798\n",
      "Epoch 10/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0383 - accuracy: 0.9884 - val_loss: 0.0203 - val_accuracy: 0.9936\n",
      "Epoch 11/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0199 - accuracy: 0.9928 - val_loss: 0.0191 - val_accuracy: 0.9934\n",
      "Epoch 12/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.0189 - val_accuracy: 0.9939\n",
      "Epoch 13/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0227 - val_accuracy: 0.9928\n",
      "Epoch 14/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0220 - accuracy: 0.9933 - val_loss: 0.0526 - val_accuracy: 0.9782\n",
      "Epoch 15/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.0176 - val_accuracy: 0.9939\n",
      "Epoch 16/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0118 - accuracy: 0.9957 - val_loss: 0.0199 - val_accuracy: 0.9939\n",
      "Epoch 17/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0818 - val_accuracy: 0.9798\n",
      "Epoch 18/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0217 - val_accuracy: 0.9936\n",
      "Epoch 19/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.0257 - val_accuracy: 0.9928\n",
      "Epoch 20/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0202 - accuracy: 0.9944 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
      "Epoch 21/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0252 - val_accuracy: 0.9920\n",
      "Epoch 22/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0317 - val_accuracy: 0.9931\n",
      "Epoch 23/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0216 - val_accuracy: 0.9936\n",
      "Epoch 24/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0235 - val_accuracy: 0.9934\n",
      "Epoch 25/25\n",
      "941/941 [==============================] - 3s 3ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0192 - val_accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Sauvegarder le modèle et les objets associés\n",
    "joblib.dump(model, \"nn_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a8549-5eed-4da1-8e08-626c85fdf4ff",
   "metadata": {},
   "source": [
    "7. Évaluer le modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af872c57-3a6d-4826-a3d9-0c971ca11606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parlez maintenant...\n",
      "Enregistrement terminé.\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Probabilité prédite : 0.5771\n",
      "Mot détecté : eteindre\n"
     ]
    }
   ],
   "source": [
    "def predict_live_audio(model, scaler, label_encoder, sr=16000, duration=2, max_pad_len=100):\n",
    "    try:\n",
    "        # Enregistrer un audio via le microphone\n",
    "        print(\"Parlez maintenant...\")\n",
    "        audio = sd.rec(int(duration * sr), samplerate=sr, channels=1)\n",
    "        sd.wait()\n",
    "        print(\"Enregistrement terminé.\")\n",
    "        \n",
    "        # Extraire les MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=audio.flatten(), sr=sr, n_mfcc=13)\n",
    "        pad_width = max_pad_len - mfcc.shape[1]\n",
    "        if pad_width > 0:\n",
    "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_pad_len]\n",
    "        \n",
    "        # Transformer les caractéristiques\n",
    "        features = mfcc.flatten().reshape(1, -1)\n",
    "        features = scaler.transform(features)\n",
    "        \n",
    "        # Faire la prédiction avec le modèle\n",
    "        probability = model.predict(features)[0][0]\n",
    "        print(f\"Probabilité prédite : {probability:.4f}\")\n",
    "        \n",
    "        # Appliquer un seuil de confiance\n",
    "        if probability < 0.5:\n",
    "            print(\"Mot détecté : Inconnu\")\n",
    "        else:\n",
    "            predicted_label = int(probability > 0.5)\n",
    "            predicted_word = label_encoder.inverse_transform([predicted_label])[0]\n",
    "            print(f\"Mot détecté : {predicted_word}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la reconnaissance : {e}\")\n",
    "\n",
    "# Charger le modèle et les objets sauvegardés\n",
    "model = joblib.load(\"nn_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Tester la reconnaissance en temps réel\n",
    "predict_live_audio(model, scaler, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dbf35-d4f3-43d8-8486-c6104e9336f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
